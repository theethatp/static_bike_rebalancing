# -*- coding: utf-8 -*-
"""Neural Combinatorial solving Bike Rebalancing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VcCXtiiy8oP_sU-xDCoBLdb0WxWAgDf8

## Import Library and Config
"""

#-*- coding: utf-8 -*-
import argparse
import ast
import collections
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from tqdm import tqdm
import statistics


parser = argparse.ArgumentParser(description='Configuration file')
arg_lists = []


def add_argument_group(name):
    arg = parser.add_argument_group(name)
    arg_lists.append(arg)
    return arg


def str2bool(v):
    return v.lower() in ('true', '1')


# Network
net_arg = add_argument_group('Network')
net_arg.add_argument('--input_embed', type=int, default=256, help='actor input embedding')
net_arg.add_argument('--hidden_dim', type=int, default=256, help='actor LSTM num_neurons')

# Data
data_arg = add_argument_group('Data')
data_arg.add_argument('--batch_size', type=int, default=1, help='batch size') 
data_arg.add_argument('--input_dimension', type=int, default=2, help='city dimension')
data_arg.add_argument('--dir_', type=str, default='n20c20', help='Random instances') # directory
data_arg.add_argument('--num_stations', type=int, default=20, help='number of stations') # this excludes depot
data_arg.add_argument('--vehicle_cap', type=int, default=10, help='number of vehicle capacity') ###############
data_arg.add_argument('--station_cap', type=int, default=20, help='number of station capacity') ###############

# Training / test parameters
train_arg = add_argument_group('Training')
train_arg.add_argument('--nb_epoch', type=int, default=10000, help='nb epoch')
train_arg.add_argument('--nb_test', type=int, default=5000, help='nb test')
train_arg.add_argument('--lr1_start', type=float, default=0.001, help='actor learning rate')
train_arg.add_argument('--lr1_decay_step', type=int, default=5000, help='lr1 decay step')
train_arg.add_argument('--lr1_decay_rate', type=float, default=0.96, help='lr1 decay rate')

# train_arg.add_argument('--beta', type=int, default=10, help='weight for TW constraint') ###################### 3 during training / 10 for test
train_arg.add_argument('--beta', type=int, default=10, help='weight for TW constraint') ###################### 3 during training / 10 for test
train_arg.add_argument('--temperature', type=float, default=1.0, help='pointer_net initial temperature') #####
train_arg.add_argument('--C', type=float, default=10.0, help='pointer_net tan clipping')

# Misc
misc_arg = add_argument_group('User options') #####################################################

misc_arg.add_argument('--pretrain', type=str2bool, default=False, help='faster datagen for infinite speed')
misc_arg.add_argument('--inference_mode', type=str2bool, default=False, help='switch to inference mode when model is trained')
misc_arg.add_argument('--restore_model', type=str2bool, default=False, help='whether or not model is retrieved')

misc_arg.add_argument('--save_to', type=str, default='speed10/s10_k5_n20w100', help='saver sub directory') #####################
misc_arg.add_argument('--restore_from', type=str, default='speed10/s10_k5_n20w100', help='loader sub directory') ###############
misc_arg.add_argument('--log_dir', type=str, default='summary/test', help='summary writer log directory') 



def get_config():
    config, unparsed = parser.parse_known_args()
    return config, unparsed


def print_config():
    config, _ = get_config()
    print('\n')
    print('Data Config:')
    print('* Batch size:',config.batch_size)
    print('* Number of Stations:',config.num_stations)
    print('* City coordinates:',config.input_dimension)
    print('\n')
    print('Network Config:')
    print('* Restored model:',config.restore_model)
    print('* Actor input embedding:',config.input_embed)
    print('* Actor hidden_dim (num neurons):',config.hidden_dim)
    print('* Actor tan clipping:',config.C)
    print('\n')
    if config.inference_mode==False:
        print('Training Config:')
        print('* Nb epoch:',config.nb_epoch)
        print('* Temperature:',config.temperature)
        print('* Actor learning rate (init,decay_step,decay_rate):',config.lr1_start,config.lr1_decay_step,config.lr1_decay_rate)
    else:
        print('Testing Config:')
        print('* Summary writer log dir:',config.log_dir)
        print('\n')

"""Encoder"""

class Encoder(tf.keras.layers.Layer):

    def __init__(self, config):
        super(Encoder,self).__init__()
        
        self.config=config

        # Data config
        self.batch_size = config.batch_size # batch size
        self.num_stations = config.num_stations # input sequen~ce length (number of cities)
        self.input_dimension = config.input_dimension # dimension of a city (coordinates)

        # Network config
        self.input_embed = config.input_embed # dimension of embedding space
        self.num_neurons = config.hidden_dim # dimension of hidden states (LSTM cell)
        self.initializer = tf.keras.initializers.GlorotUniform() # variables initializer
        
        self.W_embed = self.add_weight(shape=[1,self.input_dimension+2, self.input_embed], initializer='glorot_uniform', trainable=True, dtype=tf.float32)

        self.embedded_input = tf.keras.layers.Conv1D(   
                                                        filters = self.input_embed,
                                                        kernel_size = 1,
                                                        strides=1, 
                                                        padding='valid', 
                                                        kernel_initializer=self.initializer,
                                                        name="embedded_input"
                                                    )
        self.layer_norm = tf.keras.layers.BatchNormalization(axis=2)
        # Encode input sequence
        # Return the output activations [Batch size, Sequence Length, Num_neurons] and last hidden state as tensors.
        self.lstm = tf.keras.layers.LSTM(
                                            self.num_neurons, 
                                            kernel_initializer=self.initializer,
                                            return_sequences=True, 
                                            return_state=True
                                        )
    
    # input dimension : [Batch Size, Sequence Length, Features]
    def call(self, input_, encoder_mode):
        input_=tf.cast(input_, tf.float32)
        x = tf.matmul(input_, self.W_embed)
        x = tf.squeeze(x, axis=0)
        x = self.embedded_input(x)
        x = self.layer_norm(x)
        whole_seq_output, final_encoder_output, final_encoder_state  = self.lstm(x)

        if encoder_mode == 'critic' :
            frame = tf.reduce_mean(whole_seq_output, 1) # [Batch size, Sequence Length, Num_neurons] to [Batch size, Num_neurons]
            frame = final_encoder_state[0] 
            return whole_seq_output, final_encoder_output, final_encoder_state, frame

        return whole_seq_output, final_encoder_output, final_encoder_state

"""Attention"""

class Attention(tf.keras.layers.Layer):
    def __init__(self, n_hidden, config):
        super(Attention, self).__init__()

        ############################
        ########## Config ##########
        ############################

        self.inference_mode = config.inference_mode # True for inference, False for training
        self.temperature = config.temperature # temperature parameter
        self.C = config.C # logit clip
        self.n_hidden = n_hidden
        self.initializer = tf.keras.initializers.GlorotUniform()

        ######################################
        ########## Decoder's output ##########
        ######################################

        self.log_softmax = [] # store log(p_theta(pi(t)|pi(<t),s)) for backprop
        self.positions = [] # store visited cities for reward
        self.attending = [] # for vizualition
        self.pointing = [] # for vizualition
        self.encoded_ref_g = tf.keras.layers.Conv1D(filters=self.n_hidden, kernel_size=1, strides=1, padding='valid', kernel_initializer=self.initializer, name="encoded_ref_g") # [Batch size, seq_length, n_hidden]
        self.encoded_ref = tf.keras.layers.Conv1D(filters=self.n_hidden, kernel_size=1, strides=1, padding='valid', kernel_initializer=self.initializer, name="encoded_ref")
    
        # Attending mechanism
        self.W_q_g = self.add_weight(shape=[self.n_hidden,self.n_hidden], initializer='glorot_uniform', trainable=True)
        self.v_g = self.add_weight(shape=[self.n_hidden], initializer='glorot_uniform', trainable=True)

        # Pointing mechanism
        self.W_q = self.add_weight(shape=[self.n_hidden,self.n_hidden], initializer='glorot_uniform', trainable=True)
        self.v = self.add_weight(shape=[self.n_hidden], initializer='glorot_uniform', trainable=True)

    def call(self, ref, query, encoder_mode):

        # Attending mechanism
        encoded_ref_g = self.encoded_ref_g(ref)
        encoded_query_g = tf.expand_dims(tf.matmul(query, self.W_q_g, name="encoded_query_g"), 1) # [Batch size, 1, n_hidden]
        scores_g = tf.reduce_sum(self.v_g * tf.tanh(encoded_ref_g + encoded_query_g), [-1], name="scores_g") # [Batch size, seq_length]

        # Attend to current city and cities to visit only (Apply mask)
        attention_g = tf.nn.softmax(scores_g,name="attention_g")
        self.attending.append(attention_g)

        # 1 glimpse = Linear combination of reference vectors (defines new query vector)
        glimpse = tf.multiply(ref, tf.expand_dims(attention_g,2))
        if encoder_mode == 'observe':
            glimpse = tf.reduce_sum(glimpse,1) + query
        elif encoder_mode == 'critic' : 
            glimpse = tf.reduce_sum(glimpse,1)
            return glimpse

        # Pointing mechanism with 1 glimpse
        encoded_ref = self.encoded_ref(ref) # [Batch size, seq_length, n_hidden]
        encoded_query = tf.expand_dims(tf.matmul(glimpse, self.W_q, name="encoded_query"), 1) # [Batch size, 1, n_hidden]
        scores = tf.reduce_sum(self.v * tf.tanh(encoded_ref + encoded_query), [-1], name="scores") # [Batch size, seq_length]
        if self.inference_mode == True:
            scores = scores/self.temperature # control diversity of sampling (inference mode)
        scores = self.C*tf.tanh(scores) # control entropy

        # Point to cities to visit only (Apply mask)
        masked_scores = scores # [Batch size, seq_length]
        pointing = tf.nn.softmax(masked_scores, name="attention") # [Batch size, Seq_length]
        self.pointing.append(pointing)
        
        return masked_scores, attention_g, pointing

"""Decoder

โค้ด overlap กับ attention
"""

# RNN decoder for pointer network
class Decoder(tf.keras.layers.Layer):
    def __init__(self, n_hidden, config):
        super(Decoder, self).__init__()
        self.inference_mode = config.inference_mode # True for inference, False for training
        self.temperature = config.temperature # temperature parameter
        self.C = config.C # logit clip
        self.seq_length = config.num_stations

        self.initializer = tf.keras.initializers.GlorotUniform()

        ##########################################
        ########## Decoder's parameters ##########
        ########################################## 

        # Decoder LSTM cell
        self.cell = tf.keras.layers.LSTM(   
                                            n_hidden, 
                                            kernel_initializer=self.initializer,
                                            return_state=True
                                        )

    def call(self, prev_ouput, prev_state):

        #######################################
        ########## Reference vectors ##########
        #######################################
        h = prev_ouput
        c = prev_state

        prev_ouput = tf.expand_dims(prev_ouput, axis=0)
        # Run the cell on a combination of the 
        _, final_encoder_output, final_encoder_state = self.cell(prev_ouput, initial_state = [h,c])

        return final_encoder_output, final_encoder_state

# Tensor summaries for TensorBoard visualization
def variable_summaries(name,var, with_max_min=False):
  with tf.name_scope(name):
    mean = tf.reduce_mean(var)
    tf.summary.scalar('mean', mean)
    with tf.name_scope('stddev'):
      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
    tf.summary.scalar('stddev', stddev)
    if with_max_min == True:
        tf.summary.scalar('max', tf.reduce_max(var))
        tf.summary.scalar('min', tf.reduce_min(var))

"""ffn"""

class ffn(tf.keras.layers.Layer):

    def __init__(self, config):
        super(ffn, self).__init__()
        tf.keras.mixed_precision.set_global_policy('float32')
        ############################
        ########## Config ##########
        ############################

        self.num_stations = config.num_stations # input sequence length (number of cities)
        
        # Network config
        self.input_embed = config.input_embed # dimension of embedding space
        self.num_neurons = config.hidden_dim # dimension of hidden states (LSTM cell)
        self.initializer = 'glorot_uniform' # variables initializer

        # Initializer setup
        initializer = tf.keras.initializers.GlorotUniform()

        # Baseline setup
        self.init_baseline = self.num_stations/2 # good initial baseline for TSP

        #ffn 1
        self.h0 = tf.keras.layers.Dense(self.num_neurons, activation='relu', kernel_initializer=initializer)
        
        # ffn 2
        self.w1 = self.add_weight(shape=(self.num_neurons, 1), initializer='glorot_uniform', trainable=True, name="w1")
        self.b1 = tf.Variable([[self.init_baseline]], trainable=True, name="b1")

    def call(self, glimpse):

        h0 = self.h0(glimpse)
        
        predictions = tf.squeeze(tf.matmul(h0, self.w1) + self.b1)

        return predictions

"""Model (Actor Critic Network)

- ฟังก์ชันนี้ต้องรับ Input
- นำเข้า encoder
- ส่งไปที่ decoder
- ส่งไปที่ attention
"""

class ActorCritic(tf.keras.Model):

    def __init__(self, config):
        super(ActorCritic, self).__init__()
        self.config=config

        #######################################
        ########## Reference vectors ##########
        #######################################

        # Data config
        self.batch_size = 1 # batch size
        self.num_stations = config.num_stations # input sequence length (number of cities)
        self.input_dimension = config.input_dimension # dimension of a city (coordinates)
        
        # Network config
        self.input_embed = config.input_embed # dimension of embedding space
        self.num_neurons = config.hidden_dim # dimension of hidden states (LSTM cell)
        self.initializer = 'glorot_uniform' # variables initializer

        # Reward config
        self.beta = config.beta # penalty for constraint

        # Training config (actor)
        self.global_step = tf.Variable(0, trainable=False, name="global_step") # global step
        self.lr1_start = config.lr1_start # initial learning rate
        self.lr1_decay_rate = config.lr1_decay_rate # learning rate decay rate
        self.lr1_decay_step = config.lr1_decay_step # learning rate decay step
        self.is_training = not config.inference_mode

        # Training config (critic)
        self.global_step2 = tf.Variable(0, trainable=False, name="global_step2") # global step
        self.lr2_start = config.lr1_start # initial learning rate
        self.lr2_decay_rate= config.lr1_decay_rate # learning rate decay rate
        self.lr2_decay_step= config.lr1_decay_step # learning rate decay step
    
        # Tensor block holding the input sequences [Batch Size, Sequence Length, Features]
        self.input_ = tf.keras.Input(dtype=tf.float32, shape=(self.batch_size, self.num_stations+1, self.input_dimension+2), name="input_raw")  # +1 for depot / +2 for TW mean and TW width

        # Encoder
        self.encoder = Encoder(self.config)
        # Decoder
        self.decoder = Decoder(self.num_neurons, self.config)
        # Attention
        self.attention = Attention(self.num_neurons, self.config)
        # ffn 1
        self.ffn = ffn(self.config)
        self.distr = tf.compat.v1.distributions
        
    def call(self, inputs):
        ## Decoder's output 
        self.log_softmax = [] # store log(p_theta(pi(t)|pi(<t),s)) for backprop
        self.positions = [] # store visited cities for reward
        self.attending = [] # for vizualition
        self.pointing = [] # for vizualition
        # encoder
        self.encoder_mode = 'observe'
        whole_seq_output, final_encoder_output, final_encoder_state = self.encoder(inputs, self.encoder_mode)
         # Tensor [Batch size x time steps x cell.state_size] to attend to

        # print(final_encoder_seq_output.shape)
        # print(encoder_output.shape)
        # print(final_encoder_state.shape)
        self.h = tf.transpose(whole_seq_output, [1, 0, 2]) # [time steps x Batch size x cell.state_size]

        batch_size = whole_seq_output.get_shape().as_list()[0] # batch size
        input_length = whole_seq_output.get_shape().as_list()[1] # sequence length
        n_hidden = whole_seq_output.get_shape().as_list()[2] # num_neurons
        # print(batch_size, input_length, n_hidden)

        # Start from depot
        self.depot_position = tf.constant(0,shape=[batch_size])
        # Decoder initial input is depot (start)
        decoder_first_input = tf.gather(self.h, self.depot_position)[0]

        # decoder_initial_state: Tuple Tensor (c,h) of size [batch_size x cell.state_size]
        # decoder_first_input: Tensor [batch_size x cell.state_size]

        # Loop the decoding process and collect results
        prev_input,prev_state = decoder_first_input, final_encoder_state
        
        max_length = self.num_stations*3
        
        # Loop the decoding process and collect results
        for t in tf.range(max_length):
            # Run the cell on a combination of the previous input and state
            final_encoder_output, final_encoder_state = self.decoder(prev_input,prev_state)
            # Attention mechanism
            masked_scores, attention_g, pointing = self.attention(whole_seq_output, final_encoder_state, self.encoder_mode)
            self.attending.append(attention_g)
            self.pointing.append(pointing)

            # Multinomial distribution
            prob = self.distr.Categorical(masked_scores)

            # Sample from distribution
            # position = tf.random.categorical(prob, 1, dtype=tf.int32)
            position = prob.sample()
            position = tf.cast(position,tf.int32)
            
            self.positions.append(position)

            # Store log_prob for backprop
            self.log_softmax.append(prob.log_prob(position))

            # Update current city and mask
            self.current_city = tf.one_hot(position, self.num_stations)
            # self.mask = self.mask + self.current_city

            # Retrieve decoder's new input
            prev_input = final_encoder_output
            prev_state = final_encoder_state

            if position==0 or t==max_length-1: 
                finish_station = self.positions[-1]
                if tf.math.not_equal(finish_station, 0) : self.positions.append(self.depot_position)
                break # done

        # Return to depot
        self.positions.insert(0, self.depot_position)

        # Stack visited indices
        self.positions=tf.stack(self.positions,axis=1)  # [Batch,seq_length+1]

        # Sum log_softmax over output steps
        self.log_softmax=tf.add_n(self.log_softmax)  # [Batch,seq_length-1]

        # Stack attending & pointing distribution
        self.attending=tf.stack(self.attending,axis=1) # [Batch,seq_length-1,seq_length]
        self.pointing=tf.stack(self.pointing,axis=1) # [Batch,seq_length-1,seq_length]
        
        return self.positions, self.log_softmax, self.attending, self.pointing

"""Environment"""

import networkx as nx
from networkx.algorithms.flow import shortest_augmenting_path
# import tensorflow as tf

"""
positions : a sequence of vertices indexed with number of its occurrence or its dimension as follows
            [batch x seq. length x [sta. no., dimension + 2]]
             or [[[station no. , initial state, final state], ..., [station no., initial state, final state]]]
p' : the value of the flow in the arc btw. s and the first  occurrence of i 
q' : the value of the flow in the arc btw. the last occurrence of i and t
"""

# to get imbalance of the problem
def solve_b_flow(config, positions, state):
    
    # print('depot :', depot_state)
    # print('state :', states)
    # print('position :', positions)

    # getting a dictionary of arcs that are selected
    # extracting instances dictionary
    num_stations = config.num_stations # start with 20
    veh_cap = config.vehicle_cap # vehicle capacity = 10
    station_cap = config.station_cap # station capacity = 20

    p_prime = np.array([0 for i in range(num_stations+1)])
    q_prime = np.array([0 for i in range(num_stations+1)])

    # print('depot :', depot_state.shape)
    # print('state :', states.shape)
    # print('position :', positions.shape)
    # states_ = np.concatenate((states), axis=1)

    seq = positions.numpy()[0]
    state =state.numpy()

    all_stations_set = np.arange(num_stations+1, dtype=int) # Including depot station
    
    # creating dictionary for collecting number+1 of node present in graph
    t = {i : np.count_nonzero(positions==i) for i in range(num_stations+1)}
    # Creating incomplete set
    nodes = {'s','t'}
    edges_to_cap = {}

    # The arcs in A' are of 4 types :
    for i in all_stations_set:   # (1) btw. s and first occurrence of each vertex i
        edges_to_cap[('s',(i,1))] = state[0,i,2]
        nodes.add((i,1))

    for i in range(len(seq)-1): # (2) (i[j],i[j+1]) for each j = 1,...,k-1
        edges_to_cap[((seq[i],t[seq[i]]),(seq[i+1],t[seq[i+1]]))] = veh_cap
        nodes.add((seq[i],t[seq[i]]))
        nodes.add((seq[i+1],t[seq[i+1]]))
        t[seq[i]] += 1
    t[seq[i+1]] += 1

    for key, value in t.items(): # (3) btw. the rth occurrence of each vertex i and its (r+1)th occurrence
        for t_ in range(1,value-1):
            edges_to_cap[((key,t_),(key,t_+1))] = station_cap

    for i in all_stations_set: # (4) btw. the last occurence of each vertex i in the sequence and t
        edges_to_cap[(i,t[i]-1),'t'] = state[0,i,3]

    not_present_node = dict(filter(lambda elem: elem[1]==1, t.items())) # vertex which are not presented in the sequence
    for i in not_present_node.keys():
        edges_to_cap['s',(i,1)] = min(state[0,i,2], state[0,i,3])
        edges_to_cap[(i,1),'t'] = min(state[0,i,2], state[0,i,3])
        nodes.add((i,t[i]))
        t[i] += 1

    G = nx.DiGraph() # create directed graph        
    for (i,j),key in edges_to_cap.items():
        G.add_edge(i,j,capacity=key)
    _, flow_dict = nx.maximum_flow(G,'s','t',flow_func=shortest_augmenting_path) # compute the maximum flow problem

    # preflow_push(G, "s", "t", value_only=True)
    successors_from_s = [succ for succ in G.successors('s')]
    predecessors_to_t = [pred for pred in G.predecessors('t')]
    
    p_prime = np.array([flow_dict['s'][(i,t)] for (i,t) in successors_from_s])
    p_prime = my_func(p_prime)
    q_prime = np.array([flow_dict[(i,t)]['t'] for (i,t) in predecessors_to_t])
    q_prime = my_func(q_prime)

    tmp = 0
    for i in range(len(positions)-1):
        if positions[i]==positions[i+1]: tmp +=1

    imbalance = np.sum(np.abs(state[0,1:,2]-p_prime[1:]))+tmp*2

    return imbalance

"""## DataGenerator"""

def transform_txt_to_dict(dict):
    tmp_dict = {}
    for k,v in dict.items():
        tmp_dict[int(k)]=int(v)
    tmp_dict = collections.OrderedDict(sorted(tmp_dict.items()))
    return tmp_dict

def my_func(arg):
  arg = tf.convert_to_tensor(arg, dtype=tf.int32)
  return arg

class DataGenerator(object):

    # Initialize a DataGenerator
    def __init__(self, config):
        self.batch_size = config.batch_size
        self.dimension = config.input_dimension
        self.num_stations = config.num_stations
    
    def load_data(self, test_mode=True):
        # load x,y coordination
        with open("/content/drive/MyDrive/branch_and_cut/20B/XY.txt", "r") as data:
            tmp1,tmp2 = data.read().split('\n')
            dictionary1 = ast.literal_eval(tmp1)
            dictionary2 = ast.literal_eval(tmp2)
        X = np.fromiter(dictionary1.values(), dtype=float) # numpy of X
        Y = np.fromiter(dictionary2.values(), dtype=float) # numpy of Y

        if test_mode:
            with open("/content/drive/MyDrive/branch_and_cut/20B/test_instance20.txt", "r") as data:
                tmp1,tmp2 = data.read().split('\n')
                dictionary1 = ast.literal_eval(tmp1)
                dictionary2 = ast.literal_eval(tmp2)
            p_dict = transform_txt_to_dict(dictionary1)
            q_dict = transform_txt_to_dict(dictionary2)
            P = np.fromiter(p_dict.values(), dtype=float) # numpy of initial state
            Q = np.fromiter(q_dict.values(), dtype=float) # numpy of final state
            input_ = np.stack([X,Y,P,Q], axis=1)
            input_ = np.expand_dims(input_, 0)
            input_ = np.expand_dims(input_, 0)
            input_ = my_func(input_)
            return input_
        
        else:
            while True:
                # initial_state = 10*tf.ones(shape=(self.num_stations+1, 1))
                initial_state = 10*np.ones((self.num_stations,))
                # final_state = 10*np.ones(shape=(self.num_stations+1, 1)) + np.random.randint(-10,10, size=(self.num_stations+1, 1))
                final_state = 10*np.ones((self.num_stations,)) + \
                    np.random.randint(-10, 10, (self.num_stations,))          
                if initial_state.sum() - final_state.sum() == 0:
                    break
            initial_state = np.concatenate([np.array([0]), initial_state], axis=0)
            final_state = np.concatenate([np.array([0]), final_state], axis=0)
            input_ = np.stack([X,Y,initial_state,final_state], axis=1)
            input_ = np.expand_dims(input_, 0)
            input_ = np.expand_dims(input_, 0)
            input_ = my_func(input_)
            return input_

    # Generate random TSP-PDP instance
    def gen_instance(self, test_mode=True, seed=0):
        if seed!=0: np.random.seed(seed)
        # Randomly generate (max_length+1) city integer coordinates in [0,100[      # Rq: +1 for depot
        # sequence = np.random.randint(-500,500, size=(self.num_stations+1, self.dimension))
        sequence = np.random.randint(-500, 500, (self.num_stations, self.dimension))
        while True:
            # initial_state = 10*tf.ones(shape=(self.num_stations+1, 1))
            initial_state = 10*np.ones((self.num_stations, 1))
            # final_state = 10*np.ones(shape=(self.num_stations+1, 1)) + np.random.randint(-10,10, size=(self.num_stations+1, 1))
            final_state = 10*np.ones((self.num_stations, 1)) + \
                 np.random.randint(-10, 10, (self.num_stations, 1))          
            if initial_state.sum() - final_state.sum() == 0:
                break
        input_ = np.concatenate([sequence,initial_state,final_state], axis=1)
        input_ = np.concatenate([np.array([[0,0,0,0]]), input_], axis=0)
        input_ = np.expand_dims(input_, 0)
        input_ = my_func(input_)

        if test_mode == True:
            return input_
        else:
            return input_
    
    # Generate random batch for testing procedure
    def train_batch(self):
        for _ in range(self.batch_size):
            # Generate random TSP-PDP instance
            input_ = self.gen_instance(test_mode=False)
            input_ = tf.expand_dims(input_, 0)
            # Store batch
        return input_

    def test_batch(self):
        # Generate random TSP-TW instance
        input_ = self.gen_instance(test_mode=True)
        # Store batch
        input_batch = np.tile(input_,(self.batch_size,1,1))
        return input_batch

"""## Actor-Critic"""

class Agent:
    
    def __init__(
        self, 
        config):
        """Initialize."""
        self.config= config

        # Data config
        self.batch_size = config.batch_size # batch size
        self.input_dimension = config.input_dimension # dimension of a city (coordinates)
        self.num_stations = config.num_stations # number of total stations

        # Network config
        self.input_embed = config.input_embed # dimension of embedding space
        self.num_neurons = config.hidden_dim # dimension of hidden states (LSTM cell)

        # Reward config
        self.beta = config.beta # penalty for constraint

        # Training config (actor)
        self.global_step = tf.Variable(0, trainable=False, name="global_step") # global step
        self.lr1_start = config.lr1_start # initial learning rate
        self.lr1_decay_rate = config.lr1_decay_rate # learning rate decay rate
        self.lr1_decay_step = config.lr1_decay_step # learning rate decay step
        self.is_training = not config.inference_mode

        self.actor_critic = ActorCritic(config)
        # Actor learning rate
        self.lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(self.lr1_start, self.lr1_decay_step, self.lr1_decay_rate, staircase=False, name='learning_rate1')
        # Optimizer 
        self.opt = tf.keras.optimizers.Adam(learning_rate=self.lr_schedule)
        # compile sets the training parameters
        self.actor_critic.compile(optimizer=self.opt)
    
    def build_permutation(self, input_):
        # build permutation
        self.input_ = input_
        self.positions, self.log_softmax, self.attending, self.pointing = self.actor_critic(input_)

    def save_models(self):
        print('... saving models ...')
        self.actor_critic.save("my_model")
        # self.actor_critic.save_weights("/checkpoints/my_model")
        # saved_model_path = '/tmp/tf_save'
        # tf.saved_model.save(self.actor_critic, saved_model_path)

    def load_models(self):
        print('... loading models ...')
        self.actor_critic.load_model("/checkpoints/my_model")
        
    # Compute a sequence's reward
    def build_reward(self):

        # Convert sequence to tour (end=start)
        self.permutations = self.positions
        self.input_ = tf.squeeze(self.input_, 0)
        self.ordered_input = tf.gather(self.input_, indices=self.positions, batch_dims=1)

        # Compute tour length
        inter_city_distances = np.sum(np.sqrt(np.sum(np.square(self.ordered_input[0,:-1,:2] \
            -self.ordered_input[0,1:,:2]), axis=1)))
        self.distances = np.sum(inter_city_distances)
        
        # Compute delivery times at each city and count late cities
        self.imbalance = solve_b_flow(self.config, self.positions, self.input_)
    
    def train_step(self,input_):
        
        with tf.GradientTape(persistent=True) as tape:
            # build permutation
            self.input_ = input_
            self.positions, self.log_softmax, self.attending, self.pointing = self.actor_critic(input_)
            self.build_reward()
            temp1 = tf.tile(self.input_[:,0,:2],(self.num_stations,1))
            temp2 = self.input_[0,1:,:2]
            temp1 = tf.cast(temp1, tf.float32)
            temp2 = tf.cast(temp2, tf.float32)
            self.dept_distance = tf.reduce_sum(tf.sqrt(tf.reduce_sum(tf.square(temp1-temp2), axis=1)), axis=0)/float(self.num_stations)

            # Define reward from tour length & delay
            self.reward = tf.cast(self.distances,tf.float32)+self.beta*self.dept_distance*self.imbalance
            
            # encoder
            self.input_ = tf.expand_dims(self.input_, axis=0)
            self.encoder_mode = 'critic'
            whole_seq_output, final_encoder_output, final_encoder_state, frame = self.actor_critic.encoder(self.input_, self.encoder_mode)

            # glimpse
            frame = tf.expand_dims(frame, 0)
            glimpse = self.actor_critic.attention(whole_seq_output, frame, self.encoder_mode)

            # prediction
            self.critic = self.actor_critic.ffn(glimpse)

            # Reinforce
            # Discounted reward
            self.reward_baseline = tf.stop_gradient(self.reward - self.critic) # [Batch size, 1]
            variable_summaries('reward_baseline',self.reward_baseline, with_max_min = True)
            # Loss
            self.actor_loss = tf.reduce_mean(self.reward_baseline*self.log_softmax,0)
            tf.summary.scalar('actor loss :', self.actor_loss)
            # State-Value
            # Loss
            self.critic_loss = tf.math.square(self.reward-self.critic)
            tf.summary.scalar('critic loss :', self.critic_loss)
            # Total Loss
            self.total_loss = (self.actor_loss + self.critic_loss)

        # Minimize step
        params = self.actor_critic.trainable_variables
        gvs = tape.gradient(self.total_loss, params)
        capped_gvs = [(tf.clip_by_norm(grad, 1.)) for grad in gvs if grad is not None] # L2 clip
        self.actor_critic.optimizer.apply_gradients(zip(capped_gvs, self.actor_critic.trainable_variables))

    def active_search(self, input_):

        with tf.GradientTape(persistent=True) as tape:
            # build permutation
            self.input_ = input_
            self.positions, self.log_softmax, self.attending, self.pointing = self.actor_critic(input_)
            self.build_reward()
            temp1 = tf.tile(self.input_[:,0,:2],(self.num_stations,1))
            temp2 = self.input_[0,1:,:2]
            temp1 = tf.cast(temp1, tf.float32)
            temp2 = tf.cast(temp2, tf.float32)
            self.dept_distance = tf.reduce_sum(tf.sqrt(tf.reduce_sum(tf.square(temp1-temp2), axis=1)), axis=0)/float(self.num_stations)

            # Define reward from tour length & delay
            self.reward = tf.cast(self.distances,tf.float32)+self.beta*self.dept_distance*self.imbalance
            
            # encoder
            self.input_ = tf.expand_dims(self.input_, axis=0)
            self.encoder_mode = 'critic'
            whole_seq_output, final_encoder_output, final_encoder_state, frame = self.actor_critic.encoder(self.input_, self.encoder_mode)

            # glimpse
            frame = tf.expand_dims(frame, 0)
            glimpse = self.actor_critic.attention(whole_seq_output, frame, self.encoder_mode)
            
            # prediction
            self.critic = self.actor_critic.ffn(glimpse)

            # Reinforce
            # Discounted reward
            self.reward_baseline = tf.stop_gradient(self.reward - self.critic) # [Batch size, 1]
            variable_summaries('reward_baseline',self.reward_baseline, with_max_min = True)
            # Loss
            self.actor_loss = tf.reduce_mean(self.reward_baseline*self.log_softmax,0)
            tf.summary.scalar('actor loss :', self.actor_loss)

        # Minimize step
        params = self.actor_critic.trainable_variables
        gvs = tape.gradient(self.actor_loss, params)
        capped_gvs = [(tf.clip_by_norm(grad, 1.)) for grad in gvs if grad is not None] # L2 clip
        self.actor_critic.optimizer.apply_gradients(zip(capped_gvs, self.actor_critic.trainable_variables))

    def test(self, input_):
        self.input_ = input_
        self.positions, self.log_softmax, self.attending, self.pointing = self.actor_critic(input_)
        self.build_reward()
        temp1 = tf.tile(self.input_[:,0,:2],(self.num_stations,1))
        temp2 = self.input_[0,1:,:2]
        temp1 = tf.cast(temp1, tf.float32)
        temp2 = tf.cast(temp2, tf.float32)
        self.dept_distance = tf.reduce_sum(tf.sqrt(tf.reduce_sum(tf.square(temp1-temp2), axis=1)), axis=0)/float(self.num_stations)

        # Define reward from tour length & delay
        self.reward = tf.cast(self.distances,tf.float32)+self.beta*self.dept_distance*self.imbalance

"""## Run"""

from google.colab import drive
drive.mount('/content/drive')

#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Model:
# Decoder inputs = Encoder outputs
# Critic design (state value function approximator) = RNN encoder last hidden state (c) (parametric baseline ***) + 1 glimpse over (c) at memory states + 2 FFN layers (ReLu), w/o moving_baseline (init_value = 7 for TSPTW20)
# Penalty: Discrete (counts) with beta = +3 for one constraint / beta*sqrt(N) for N constraints violated (concave **0.5)
# No Regularization
# Decoder Glimpse = on Attention_g (mask - current)
# Residual connections 01

# NEW data generator (wrap config.py)
# speed1000 model: n20w100
# speed10 model: s10_k5_n20w100 (fine tuned w/ feasible kNN datagen)
# Benchmark: Dumas n20w100 instances

# Get running configuration
config, _ = get_config()
print_config()
# Build tensorflow graph from config
print("Building graph...")
agent = Agent(config) ####### VERY DANGER!!!!

load_checkpoint = False
inference_mode = False

if load_checkpoint:
    agent.load_models()
    
# Initialize data generator
training_set = DataGenerator(config)
# input_ = training_set.train_batch()
input_ = training_set.load_data(test_mode=False)

# Keep last episodes reward
episodes_reward: collections.deque = collections.deque(maxlen=config.nb_epoch)
running_episodes_reward: collections.deque = collections.deque(maxlen=config.nb_epoch)

if config.restore_model==True:
    ckpt = tf.train.Checkpoint(net=agent.actor_critic, optimizer=agent.opt)
    manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)
    print("Model restored.")

for i in tqdm(range(1,config.nb_epoch+1)):
    # Restore variables from disk.
    agent.train_step(input_)

    episodes_reward.append(agent.reward.numpy())
    running_reward = statistics.mean(episodes_reward)
    running_episodes_reward.append(running_reward)

    if i % max(1,int(config.nb_epoch/10)) == 0 and i!=0 :
        print(f'\nEpisode {i}')
        print('step_distance_and_penalty :',agent.reward.numpy(), 'average_distance_and_penalty :',running_reward)
        print('state of network is changed')
        input_ = training_set.load_data(test_mode=False)
        pass
# Save the variables to disk
# agent.save_models()
print("Training COMPLETED !")
# manager.save()

# if not load_checkpoint:
    # x = [t+1 for t in range(max_episodes)]

"""## Test Here!"""

import time

best_reward = float('inf')
best_sequence = []

reward=[]
distance=[]
imbalance=[]
sequence=[]

input_ = training_set.load_data(test_mode=True)
tic = time.perf_counter()
for t in tqdm(range(config.nb_test)):
    agent.test(input_)
    if agent.reward < best_reward:
        best_reward = agent.reward.numpy()
        best_distance = agent.distances
        best_imbalance = agent.imbalance
        best_sequence = agent.positions
        print('\nbest distance (Lowest reward) :', best_distance)
    reward.append(agent.reward.numpy())
    distance.append(agent.distances)
    imbalance.append(agent.imbalance)
    sequence.append(agent.positions)
    toc = time.perf_counter()
    if toc-tic>1200:
        break

# Save testing details to drive
tmpDict = {'reward':reward, 'distance':distance, 'imbalance':imbalance, 'sequence':sequence}
solutions=pd.DataFrame(tmpDict)
solutions.to_csv('/content/drive/MyDrive/neural_comb_results/ncrs.csv')

print("\nOverall best distance :", best_distance)
print("Imbalance :", best_imbalance)
print("Best sequence :", best_sequence)

plt.scatter(list(range(len(episodes_reward))),list(running_episodes_reward), marker='o', c='green',s=5)

# Save training details to drive
tmpDict = {'step':list(range(len(episodes_reward))), 'loss':list(episodes_reward), 'running_loss':list(running_episodes_reward)}
loss=pd.DataFrame(tmpDict)
loss.to_csv('/content/drive/MyDrive/neural_comb_results/loss.csv')

"""### Pivot

## Output
"""

training_set = DataGenerator(config)
input_ = training_set.train_batch()

agent.reward_baseline

agent.reward.numpy()

agent.train_step(input_)

agent.positions

agent.reward

agent.distances

agent.dept_distance

agent.beta*agent.dept_distance*agent.imbalance

agent.imbalance