# -*- coding: utf-8 -*-
"""tabu search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WB8ch2Qxoh-DJViDZt72yzqPe2W3OedN

# Greedy Algorithm
"""

from google.colab import drive
drive.mount('/content/drive')

def transform_seq(seq):
    '''delete consecutive in list'''
    no_consec_seq = []
    for i in range(len(seq)-1):
        if seq[i] != seq[i+1]:
            no_consec_seq.append(seq[i])
        if seq[i+1] == 0:
            no_consec_seq.append(seq[i+1])
    # new_seq=seq[np.insert(np.diff(seq).astype(np.bool), 0, True)]
    return new_seq

def transform_txt_to_dict(dict):
    tmp_dict = {}
    for k,v in dict.items():
        tmp_dict[int(k)]=int(v)
    tmp_dict = collections.OrderedDict(sorted(tmp_dict.items()))
    return tmp_dict

import ast
import numpy as np
import collections
with open("/content/drive/MyDrive/branch_and_cut/40A/XY.txt", "r") as data:
    tmp1,tmp2 = data.read().split('\n')
    dictionary1 = ast.literal_eval(tmp1)
    dictionary2 = ast.literal_eval(tmp2)
X = np.fromiter(dictionary1.values(), dtype=float) # numpy of X
Y = np.fromiter(dictionary2.values(), dtype=float) # numpy of Y

with open("/content/drive/MyDrive/branch_and_cut/40A/test_instance40.txt", "r") as data:
    tmp1,tmp2 = data.read().split('\n')
    dictionary1 = ast.literal_eval(tmp1)
    dictionary2 = ast.literal_eval(tmp2)
p_dict = transform_txt_to_dict(dictionary1)
q_dict = transform_txt_to_dict(dictionary2)
P = np.fromiter(p_dict.values(), dtype=float) # numpy of initial state
Q = np.fromiter(q_dict.values(), dtype=float) # numpy of final state
xy = np.stack([X,Y], axis=1)


from google.colab import drive
drive.mount('/content/drive')

"""# Transform"""

seq = '[ 0  2 40 13 34 12 27  9  3 14 27  1 15 20 19  5  7 15 14 19 13  3 27 10 \
  12 37 17 26  8 38  8 27 17 10 18  6 32  7 23 21 11  7 17 12 14 39 33 14 \
  25 21 22 29 30 20 30 29 10 33 22  5 33 34 24  6  3 15 23 38 36  2  9 30 \
   2 11 39 16  7 32 26 28 14 31 16 31 35 40 14  3 11 17 19 35 23 30 30 10 \
  36  6 20  4 22 32 33 12 33 23  4 13 39 16 16 16 10 13 19 39 28 27 29  7 \
  39  0 ]'

seq_list = seq.split(' ')
new_seq = []
for i in seq_list:
    try:
        new_seq.append(int(i))
    except :
        print('err')
# new_seq = [0, 5, 6, 7, 29, 8, 12, 25, 24, 36, 28, 8, 19, 20, 28, 5, 32, 27, 19, 1, 25, 2, 25, 29, 20, 8, 28, 15, 13, 30, 1, 20, 21, 35, 29, 17, 4, 16, 9, 13, 2, 40, 38, 15, 24, 32, 6, 34, 11, 19, 23, 3, 28, 9, 2, 35, 31, 38, 1, 8, 28, 30, 5, 35, 17, 8, 26, 36, 7, 26, 40, 39, 18, 34, 19, 13, 32, 34, 20, 18, 29, 30, 22, 5, 25, 28, 17, 14, 33, 37, 9, 12, 16, 6, 24, 12, 38, 14, 8, 29, 10, 35, 27, 40, 26, 16, 32, 17, 35, 6, 5, 22, 16, 8, 15, 29, 4, 0]
print(new_seq)

new_seq = transform_seq(new_seq)
print(new_seq)

from turtle import shape
import numpy as np
import random
from numba import jit,njit
import networkx as nx
from networkx.algorithms.flow import shortest_augmenting_path
import ast
import collections
import time

def transform_seq(seq):
    '''delete consecutive in list'''
    no_consec_seq = []
    for i in range(len(seq)-1):
        if seq[i] != seq[i+1]:
            no_consec_seq.append(seq[i])
        if seq[i+1] == 0:
            no_consec_seq.append(seq[i+1])
    new_seq=seq[np.insert(np.diff(seq).astype(np.bool), 0, True)]
    return new_seq

def transform_txt_to_dict(dict):
    tmp_dict = {}
    for k,v in dict.items():
        tmp_dict[int(k)]=int(v)
    tmp_dict = collections.OrderedDict(sorted(tmp_dict.items()))
    return tmp_dict

class DataGenerator(object):
    ''' Generate data from .csv or random technique'''
    # Initialize a DataGenerator
    def __init__(self):
        self.batch_size = 1
        self.dimension = 2
        self.num_stations = 40
    
    def load_data(self, test_mode=True):
        # load x,y coordination
        with open("/content/drive/MyDrive/branch_and_cut/40B/XY.txt", "r") as data:
            tmp1,tmp2 = data.read().split('\n')
            dictionary1 = ast.literal_eval(tmp1)
            dictionary2 = ast.literal_eval(tmp2)
        X = np.fromiter(dictionary1.values(), dtype=float) # numpy of X
        Y = np.fromiter(dictionary2.values(), dtype=float) # numpy of Y

        if test_mode:
        
            with open("/content/drive/MyDrive/branch_and_cut/40B/test_instance40.txt", "r") as data:
                tmp1,tmp2 = data.read().split('\n')
                dictionary1 = ast.literal_eval(tmp1)
                dictionary2 = ast.literal_eval(tmp2)
            p_dict = transform_txt_to_dict(dictionary1)
            q_dict = transform_txt_to_dict(dictionary2)
            P = np.fromiter(p_dict.values(), dtype=float) # numpy of initial state
            Q = np.fromiter(q_dict.values(), dtype=float) # numpy of final state
            input_ = np.stack([X,Y,P,Q], axis=1)
            input_ = np.expand_dims(input_, 0)
            input_ = np.expand_dims(input_, 0)
            return input_
        
        else:
            while True:
                # initial_state = 10*tf.ones(shape=(self.num_stations+1, 1))
                initial_state = 10*np.ones((self.num_stations,))
                # final_state = 10*np.ones(shape=(self.num_stations+1, 1)) + np.random.randint(-10,10, size=(self.num_stations+1, 1))
                final_state = 10*np.ones((self.num_stations,)) + \
                    np.random.randint(-10, 10, (self.num_stations,))          
                if initial_state.sum() - final_state.sum() == 0:
                    break
            initial_state = np.concatenate([np.array([0]), initial_state], axis=0)
            final_state = np.concatenate([np.array([0]), final_state], axis=0)
            input_ = np.stack([X,Y,initial_state,final_state], axis=1)
            input_ = np.expand_dims(input_, 0)
            input_ = np.expand_dims(input_, 0)
            return input_

    # Generate random TSP-PDP instance
    def gen_instance(self, test_mode=True, seed=0):
        if seed!=0: np.random.seed(seed)
        # Randomly generate (max_length+1) city integer coordinates in [0,100[      # Rq: +1 for depot
        # sequence = np.random.randint(-500,500, size=(self.num_stations+1, self.dimension))
        sequence = np.random.randint(-500, 500, (self.num_stations, self.dimension))
        while True:
            # initial_state = 10*tf.ones(shape=(self.num_stations+1, 1))
            initial_state = 10*np.ones((self.num_stations, 1))
            # final_state = 10*np.ones(shape=(self.num_stations+1, 1)) + np.random.randint(-10,10, size=(self.num_stations+1, 1))
            final_state = 10*np.ones((self.num_stations, 1)) + \
                 np.random.randint(-10, 10, (self.num_stations, 1))          
            if initial_state.sum() - final_state.sum() == 0:
                break
        input_ = np.concatenate([sequence,initial_state,final_state], axis=1)
        input_ = np.concatenate([np.array([[0,0,0,0]]), input_], axis=0)
        input_ = np.expand_dims(input_, 0)

        if test_mode == True:
            return input_
        else:
            return input_
    
    # Generate random batch for testing procedure
    def train_batch(self):
        for _ in range(self.batch_size):
            # Generate random TSP-PDP instance
            input_ = self.gen_instance(test_mode=False)
            input_ = np.expand_dims(input_, 0)
            # Store batch
        return input_

    def test_batch(self):
        # Generate random TSP-TW instance
        input_ = self.gen_instance(test_mode=True)
        # Store batch
        input_batch = np.tile(input_,(self.batch_size,1,1))
        return input_batch

def solve_b_flow(seq, state):
    ''' Get imbalance value of the given solution '''
    
    # print('depot :', depot_state)
    # print('state :', states)
    # print('position :', positions)

    # getting a dictionary of arcs that are selected
    # extracting instances dictionary
    num_stations = 40 # start with 20
    veh_cap = 10 # vehicle capacity = 10
    station_cap = 20 # station capacity = 20

    p_prime = np.zeros((num_stations+1,))
    q_prime = np.zeros((num_stations+1,))

    # print('depot :', depot_state.shape)
    # print('state :', states.shape)
    # print('position :', positions.shape)
    # states_ = np.concatenate((states), axis=1

    all_stations_set = np.arange(num_stations+1, dtype=int) # Including depot station
    
    # creating dictionary for collecting number+1 of node present in graph
    t = {i : np.count_nonzero(seq==i) for i in range(num_stations+1)}
    # Creating incomplete set
    nodes = {'s','t'}
    edges_to_cap = {}

    # The arcs in A' are of 4 types :
    for i in all_stations_set:   # (1) btw. s and first occurrence of each vertex i
        edges_to_cap[('s',(i,1))] = state[i,2]
        nodes.add((i,1))

    for i in range(len(seq)-1): # (2) (i[j],i[j+1]) for each j = 1,...,k-1
        edges_to_cap[((seq[i],t[seq[i]]),(seq[i+1],t[seq[i+1]]))] = veh_cap
        nodes.add((seq[i],t[seq[i]]))
        nodes.add((seq[i+1],t[seq[i+1]]))
        t[seq[i]] += 1
    t[seq[i+1]] += 1

    for key, value in t.items(): # (3) btw. the rth occurrence of each vertex i and its (r+1)th occurrence
        for t_ in range(1,value-1):
            edges_to_cap[((key,t_),(key,t_+1))] = station_cap

    for i in all_stations_set: # (4) btw. the last occurence of each vertex i in the sequence and t
        edges_to_cap[(i,t[i]-1),'t'] = state[i,3]

    not_present_node = dict(filter(lambda elem: elem[1]==1, t.items())) # vertex which are not presented in the sequence
    for i in not_present_node.keys():
        edges_to_cap['s',(i,1)] = min(state[i,2], state[i,3])
        edges_to_cap[(i,1),'t'] = min(state[i,2], state[i,3])
        nodes.add((i,t[i]))
        t[i] += 1

    G = nx.DiGraph() # create directed graph        
    for (i,j),key in edges_to_cap.items():
        G.add_edge(i,j,capacity=key)
    _, flow_dict = nx.maximum_flow(G,'s','t',flow_func=shortest_augmenting_path) # compute the maximum flow problem

    # preflow_push(G, "s", "t", value_only=True)
    successors_from_s = [succ for succ in G.successors('s')]
    predecessors_to_t = [pred for pred in G.predecessors('t')]
    
    p_prime = np.array([flow_dict['s'][(i,t)] for (i,t) in successors_from_s])
    q_prime = np.array([flow_dict[(i,t)]['t'] for (i,t) in predecessors_to_t])

    tmp = 0
    for i in range(len(seq)-1):
        if seq[i]==seq[i+1]: tmp +=1

    imbalance = np.sum(np.abs(state[1:,2]-p_prime[1:]))+tmp*2

    return p_prime, q_prime, imbalance

def find_inserted_arc(prev_seq,seq):
    '''Prevent the arc insert to the sequence'''
    prev_seq = prev_seq.tolist()
    seq = seq.tolist()
    prev_arc = set()
    curr_arc = set()
    for i in range(len(prev_seq)-1):
        prev_arc.add(tuple([prev_seq[i],prev_seq[i+1],i]))
    for i in range(len(seq)-1):
        curr_arc.add(tuple([seq[i],seq[i+1],i]))
    inserted_arcs = curr_arc.difference(prev_arc)
    return inserted_arcs

def in_tabu(prev_seq,seq,tabu_list):
    '''Whether the moving arc in the tabu list?'''
    inserted_arcs = find_inserted_arc(prev_seq,seq)
    for (i,j,pos) in inserted_arcs:
        tabu_list = tabu_list[:,:3]
        search_arc = np.array([i,j,pos])
        arc_loc = np.where(np.all(tabu_list==search_arc,axis=1))
        if len(arc_loc[0])!=0:
            return True
    return False

def swap(route, first, second):
    new_route=np.zeros(len(route))
    new_route[:first+1]=route[:first+1]
    new_route[first+1:second+1]=np.flip(route[first+1:second+1])
    new_route[second+1:]=route[second+1:]
    new_route = new_route.astype(int)
    return new_route

def compute_cost(seq, positions):
    permutation = np.take(positions,seq,axis=0)
    distance = np.sum(np.sqrt(np.sum(np.square(np.abs(permutation[1:,]-permutation[:-1,])),axis=1))) ##### must be refractored
    p_prime, q_prime, imbalance = solve_b_flow(seq,positions)
    return distance+10*500*1.414*(np.sum(positions[:,2]-p_prime))

def two_opt(seq, positions, tabu_list):
    best_distance=float('inf')
    best_route=seq
    for i in range(1,len(seq)-3):
    # numb = 5
    # for i in range(1,numb):
        # print(i)
        for j in range(i+2,len(seq)-1):
            new_route=swap(seq, i, j)
            # new_route=transform_seq(new_route)
            if in_tabu(seq,new_route,tabu_list):
                continue
            new_distance=compute_cost(new_route, positions)
            if new_distance<best_distance:
                print(f'Improved 2_OPT Distance : {new_distance} @ i : {i}')
                best_route=new_route
                best_distance=new_distance
    return best_distance, best_route


def suppression(seq, positions,tabu_list):
    best_sup_distance = float('inf')
    best_sup_seq = seq
    for i in range(1,len(seq)-1):
        curr_seq = np.concatenate([seq[:i],seq[i+1:]])
        # curr_seq=transform_seq(curr_seq)
        if in_tabu(seq,curr_seq,tabu_list):
            # print('In the tabu')
            continue
        curr_distance=compute_cost(curr_seq, positions)
        if curr_distance<best_sup_distance:
            print(f'Improved Suppression Distance : {curr_distance}')
            best_sup_seq = curr_seq
            best_sup_distance = curr_distance
    return best_sup_distance, best_sup_seq

def add_unbalanced(seq, positions,tabu_list):
    best_add_ubl_seq = seq
    best_add_ubl_distance = float('inf')
    p_prime, q_prime, _ = solve_b_flow(seq, positions)
    unbalance = np.abs((positions[1:,3]-q_prime[1:])-(positions[1:,2]-p_prime[1:]))
    p_minus_q = positions[1:,2]-positions[1:,3] # p-q if p-q>0 it would be excess, otherwise defaults
    excess = p_minus_q > 0
    excess = excess*1
    default = p_minus_q < 0
    default = default*1
    i = np.argmax(unbalance*excess)+1 # i
    j = np.argmax(unbalance*default)+1 # j

    i_idxs = np.where(seq==i)*1
    j_idxs = np.where(seq==j)*1

    if len(i_idxs[0]) != 0:
        for t in i_idxs[0]:
            add_ubl_seq = np.concatenate([seq[:t+1],np.array([j,i]),seq[t+1:]])
            # add_ubl_seq=transform_seq(add_ubl_seq)
            if in_tabu(seq,add_ubl_seq,tabu_list):
                # print('In the tabu')
                continue
            curr_cost = compute_cost(add_ubl_seq, positions)
            if curr_cost < best_add_ubl_distance:
                print(f'Improved Add Unbalanced Vertex Distance : {curr_cost}')
                best_add_ubl_distance = curr_cost
                best_add_ubl_seq = add_ubl_seq
    else:
        print('i is not founded')

    if len(j_idxs[0]) != 0:
        for t in j_idxs[0]:
            add_ubl_seq = np.concatenate([seq[:t+1],np.array([i,j]),seq[t+1:]])
            # add_ubl_seq=transform_seq(add_ubl_seq)
            if in_tabu(seq,add_ubl_seq,tabu_list):
                # print('In the tabu')
                continue
            curr_cost = compute_cost(add_ubl_seq, positions)
            if curr_cost < best_add_ubl_distance:
                print(f'Improved Add Unbalanced Vertex Distance : {curr_cost}')
                best_add_ubl_distance = curr_cost
                best_add_ubl_seq = add_ubl_seq
    else:
        print('j is not founded')

    if np.array_equal(best_add_ubl_seq,seq): ## i and j are not in the sequence
        add_ubl_seq = np.concatenate([seq[:-1],np.array([i,j,0])])
        # add_ubl_seq=transform_seq(add_ubl_seq)
        if not in_tabu(seq,add_ubl_seq,tabu_list):
            best_add_ubl_seq = add_ubl_seq
            best_add_ubl_distance = compute_cost(add_ubl_seq,positions)

    return best_add_ubl_distance, best_add_ubl_seq

def add_buffer(seq, positions, num_stations,tabu_list):
    best_add_buffer_seq = seq
    best_add_buffer_distance = float('inf')

    for i in range(num_stations+1):
        n = np.sum(np.where(seq==i)*1)
        if n==0:
            for t1 in range(1,len(seq)-2):
                for t2 in range(t1+1,len(seq)-1):
                    # print(f'add node {i} at {t1,t2}')
                    add_buffer_seq = np.concatenate([seq[:t1+1],np.array([i]),seq[t1+1:t2+1],np.array([i]),seq[t2+1:]])
                    # add_buffer_seq=transform_seq(add_buffer_seq)
                    if in_tabu(seq,add_buffer_seq,tabu_list):
                        continue
                    # print('In the tabu')
                    add_buffer_dist = compute_cost(add_buffer_seq,positions)
                    if add_buffer_dist<best_add_buffer_distance:
                        print(f'Improved Add Buffer Vertex Distance : {add_buffer_dist}')
                        best_add_buffer_seq = add_buffer_seq
                        best_add_buffer_distance = add_buffer_dist
        elif n==1:
            for t in range(1,len(seq)-1):
                # print(f'add node {i} at {t}')
                add_buffer_seq = np.concatenate([seq[:t+1],np.array([i]),seq[t+1:]])
                # add_buffer_seq=transform_seq(add_buffer_seq)
                if in_tabu(seq,add_buffer_seq,tabu_list):
                    # print('In the tabu')
                    continue
                add_buffer_dist = compute_cost(add_buffer_seq,positions)
                if add_buffer_dist<best_add_buffer_distance:
                    print(f'Improved Add Buffer Vertex Distance : {add_buffer_dist}')
                    best_add_buffer_seq = add_buffer_seq
                    best_add_buffer_distance = add_buffer_dist

    return best_add_buffer_distance, best_add_buffer_seq

def find_existing_arc(prev_seq, seq):
    ''' before moving existing arcs to tabu list'''
    prev_seq = prev_seq.tolist()
    seq = seq.tolist()
    prev_arc = set()
    curr_arc = set()
    for i in range(len(prev_seq)-1):
        prev_arc.add(tuple([prev_seq[i],prev_seq[i+1],i]))
    for i in range(len(seq)-1):
        curr_arc.add(tuple([seq[i],seq[i+1],i]))
    existing_arc = prev_arc.intersection(curr_arc)
    return existing_arc

def update_tabu_list(tabu_list, prev_seq, seq):
    limit = 5
    existing_arc = find_existing_arc(prev_seq, seq)
    for (a,b,i) in existing_arc:
        search_arc = np.array([[a,b,i]])
        tabu_tmp = tabu_list[:,:3]
        arc_loc = np.where(np.all(tabu_tmp==search_arc,axis=1)) # find the location of arc in tabu list
        if len(arc_loc[0])==0: ## not in tabu list
            tabu_list = np.concatenate((tabu_list,np.array([[a,b,i,0]])), axis=0)
    if len(tabu_list.shape) == 2:
        tabu_list[:,-1] = tabu_list[:,-1] + np.ones((tabu_list.shape[0],))
        idx_tabu_list = tabu_list[:,-1]<=np.ones((tabu_list.shape[0],))*limit
        print(len(idx_tabu_list.shape))
        if len(idx_tabu_list.shape)==1:
            idx_tabu_list[0] = True
            # print(idx_tabu_list.shape)
            # print(tabu_list.shape)
            tabu_list = tabu_list[idx_tabu_list,:]
        else:
            idx_tabu_list[0][0] = True
            # print(idx_tabu_list[0].shape)
            # print(tabu_list.shape)
            tabu_list = tabu_list[idx_tabu_list[0],:]
        # print('## TABU LIST ##')
        # print(tabu_list)
    return tabu_list


def is_feasible(seq,positions):
    state = positions
    imbalance = solve_b_flow(seq,state)
    if imbalance ==0:
        return True
    return False

if __name__ == '__main__':
    tic = time.perf_counter()
    consecutive_time = 0
    training_set = DataGenerator()
    input_ = training_set.load_data(test_mode=True)
    tabu_list = np.array([[100,100,0,0]], dtype=int)
    positions = input_[0][0]
    i = 1
    nb_itermax = 100
    curr_sequence = np.array(new_seq, dtype=int) # initial sequence
    best_sequence = curr_sequence # the best feasible solution
    best_distance = compute_cost(curr_sequence, positions) # the best feasible solution
    print('Initial Distance :',best_distance)
    while i<= nb_itermax:
        print(f"Iteration : {i}")
        distance_two_opt, sequence_two_opt = two_opt(curr_sequence, positions, tabu_list)
        distance_sup, sequence_sup = suppression(curr_sequence, positions,tabu_list)
        tmp_list = [sequence_two_opt,sequence_sup]
        best_nbh_sequence = tmp_list[np.argmin([distance_two_opt, distance_sup])]
        best_nbh_distance = compute_cost(best_nbh_sequence, positions)
        if not is_feasible(curr_sequence,positions):
            distance_add_unb, sequence_add_unb = add_unbalanced(curr_sequence,positions,tabu_list)
            if distance_add_unb<best_nbh_distance:
                best_nbh_sequence = sequence_add_unb
                best_nbh_distance = distance_add_unb
        a=random.random()
        if a <= 0.2:
            distance_add_buffer, sequence_add_buffer = add_buffer(curr_sequence,positions,40,tabu_list)
            if distance_add_buffer<best_nbh_distance:
                best_nbh_sequence = sequence_add_buffer
                best_nbh_distance = distance_add_buffer
        prev_sequence = curr_sequence # push variables back to the previous
        prev_distance = compute_cost(prev_sequence,positions)
        curr_sequence = best_nbh_sequence # update variables
        curr_distance = compute_cost(curr_sequence,positions)
        tabu_list = update_tabu_list(tabu_list, prev_sequence, curr_sequence)
        if not is_feasible(curr_sequence,positions) and curr_distance<compute_cost(best_sequence,positions): ##### suspiscious
            best_sequence = curr_sequence 
            best_distance = curr_distance
        if best_distance == prev_distance:
            consecutive_time += 1
            if consecutive_time == 10:
                break
        else : 
            consecutive_time = 0
        i += 1
        print("Current Best sequence :", best_sequence)
        print("Current Best sequence distance :", best_distance)
    toc = time.perf_counter()
    print("COMPLETE SEARCHING")
    print("Best sequence :", best_sequence)
    print("Best sequence distance :", best_distance)
    print("Time :", toc-tic)

def transform_seq(seq):
    '''delete consecutive in list'''
    no_consec_seq = []
    for i in range(len(seq)-1):
        if seq[i] != seq[i+1]:
            no_consec_seq.append(seq[i])
        if seq[i+1] == 0:
            no_consec_seq.append(seq[i+1])
    # new_seq=seq[np.insert(np.diff(seq).astype(np.bool), 0, True)]
    return no_consec_seq

seq ='[ 0 20 20 20 20 12 12 12 12 12 12  9  9 10 10 10 10 14  5 15 15 15 15 16\
 16  1  1 18  8 11 11  9  9  9  6  6  6  6  6  7  7  7  7 11 19  2 17  4\
  4  3  2  2  2 13 13 13  0 ]'
seq_list = seq.split(' ')
new_seq = []
for i in seq_list:
    try:
        new_seq.append(int(i))
    except :
        print('err')
print(new_seq)
seq = transform_seq(new_seq)

p_prime, q_prime, imbalance = solve_b_flow(best_sequence, positions)
    print(q_prime)
    print(positions)